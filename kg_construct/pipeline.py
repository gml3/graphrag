import logging
from typing import Any
import json
import pandas as pd

from callbacks.noop_workflow_callbacks import NoopWorkflowCallbacks
from callbacks.workflow_callbacks import WorkflowCallbacks
from config.enums import IndexingMethod
from config.models.graph_rag_config import GraphRagConfig
from index.run.utils import create_run_context
from index.typing.context import PipelineRunContext
from index.typing.pipeline import Pipeline
from index.typing.pipeline_run_result import PipelineRunResult
from utils.api import create_cache_from_config, create_storage_from_config
from index.run.utils import create_callback_chain
from index.typing.pipeline_run_result import PipelineRunResult
from index.workflows.factory import PipelineFactory
from logger.standard_logging import init_loggers

logger = logging.getLogger(__name__)


async def build_index(
    config: GraphRagConfig,
    method: IndexingMethod | str = IndexingMethod.Standard,
    callbacks: list[WorkflowCallbacks] | None = None,
    verbose: bool = False,
) -> list[PipelineRunResult]:
    """Run the pipeline with the given configuration.

    Parameters
    ----------
    config : GraphRagConfig
        The configuration.
    method : IndexingMethod default=IndexingMethod.Standard
        Styling of indexing to perform (full LLM, NLP + LLM, etc.).
    callbacks : list[WorkflowCallbacks] | None default=None
        A list of callbacks to register.
    additional_context : dict[str, Any] | None default=None
        Additional context to pass to the pipeline run. This can be accessed in the pipeline state under the 'additional_context' key.
    input_documents : pd.DataFrame | None default=None.
        Override document loading and parsing and supply your own dataframe of documents to index.

    Returns
    -------
    list[PipelineRunResult]
        The list of pipeline run results
    """
    init_loggers(config=config, verbose=verbose)

    # Create callbacks for pipeline lifecycle events if provided
    workflow_callbacks = (create_callback_chain(callbacks) if callbacks else NoopWorkflowCallbacks())

    outputs: list[PipelineRunResult] = []

    logger.info("Initializing indexing pipeline...")

    pipeline = PipelineFactory.create_pipeline(method) # 

    root_dir = config.root_dir

    input_storage = create_storage_from_config(config.input.storage) # æ‹¿åˆ°FilePipelineStorageå­˜å‚¨å¯¹è±¡
    output_storage = create_storage_from_config(config.output)  # è¾“å‡ºç»“æžœçš„å­˜å‚¨
    cache = create_cache_from_config(config.cache, root_dir)    # LLM ç¼“å­˜

    # load existing state in case any workflows are stateful
    state_json = await output_storage.get("context.json")   # è¯»å–ä¸Šæ¬¡è¿è¡Œçš„çŠ¶æ€
    state = json.loads(state_json) if state_json else {}    # æ²¡æœ‰å°±æ˜¯ç©ºå­—å…¸

    logger.info("Running standard indexing.")

    # è¿™ä¸ªå‡½æ•°æ²¡æœ‰è¯»å–ä»»ä½•æ–‡ä»¶å†…å®¹ï¼Œå®ƒåªæ˜¯æŠŠå„ç§"å·¥å…·"è£…è¿›ä¸€ä¸ªå·¥å…·ç®±ï¼ˆPipelineRunContextï¼‰ï¼Œæ–¹ä¾¿åŽç»­æ¯ä¸ª workflow ä½¿ç”¨
    context = create_run_context(
        input_storage=input_storage,         # ðŸ“‚ "è¾“å…¥æ–‡ä»¶æŸœ" â€” çŸ¥é“åŽ»å“ªå„¿è¯»åŽŸå§‹æ–‡æ¡£
        output_storage=output_storage,       # ðŸ“‚ "è¾“å‡ºæ–‡ä»¶æŸœ" â€” çŸ¥é“æŠŠç»“æžœå­˜åˆ°å“ªå„¿
        cache=cache,                         # ðŸ’¾ "LLM ç¼“å­˜"   â€” é¿å…é‡å¤è°ƒ API
        callbacks=callbacks,                 # ðŸ“¢ "é€šçŸ¥å™¨"     â€” æŠ¥å‘Šè¿›åº¦
        state=state,                         # ðŸ“‹ "çŠ¶æ€è®°äº‹æœ¬"  â€” å­˜ä¸´æ—¶ä¿¡æ¯
    )
    counter = 0
    for name, workflow_function in pipeline.run():
        context.callbacks.workflow_start(name, None)
        result = await workflow_function(config, context)
        print(result.result)
        counter += 1
        if counter > 1:
            break


if __name__ == "__main__":
    import asyncio
    from pathlib import Path
    from config.load_config import load_config

    config = load_config(root_dir=Path("."))
    asyncio.run(build_index(config))